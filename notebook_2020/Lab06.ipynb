{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNs64IofUSCTBDLs8ODx1AM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hufsaim/T10306201/blob/master/notebook/Lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU3gymLI5VA3"
      },
      "source": [
        "# Data augmentation and Implementation of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3flS45hSM1KJ"
      },
      "source": [
        "- PyTorch를 이용하여 주어진 CNN architecture를 구현하고, cifar-10 dataset에 data augmentation을 적용하여 모델을 훈련해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rw80Nho5OGA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRRjAQgN8yl"
      },
      "source": [
        "- print(device)를 통해 cuda가 출력되어 GPU를 활용할 수 있음을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIFEGu8U5dpg",
        "outputId": "d2b937b8-304f-4ebc-a7d8-e38c44423046",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_fm-eSz5s_z",
        "outputId": "e064a551-ddce-42d2-da2c-c64b7dd65969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qecrQBTQ5vtJ"
      },
      "source": [
        "path2 = '<your path>'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KqvPrMwOGrB"
      },
      "source": [
        "- torchvision의 다양한 종류의 transform들을 제공하고 있어 영상데이터의 augmentation에 활용할 수 있으며, 또한 자신만의 transform function을 만들어 활용할 수 있습니다.\n",
        "- https://pytorch.org/docs/stable/torchvision/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhvh9v2I5lLi",
        "outputId": "b42ee505-6ec6-40be-8260-739ebe0ed516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transform_train = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
        "     transforms.RandomAffine((-10,10), shear=10, scale=(0.99, 1.01)),\n",
        "     transforms.Resize((34,34)),\n",
        "     transforms.CenterCrop(32),\n",
        "     transforms.ToTensor()]\n",
        "     )\n",
        "transform_valid = transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "cifar10_train = torchvision.datasets.CIFAR10(path2, train=True, transform=transform_train, target_transform=None, download=True)\n",
        "cifar10_valid = torchvision.datasets.CIFAR10(path2, train=False, transform=transform_valid, target_transform=None, download=True)\n",
        "def get_cifar10_labels(labels):\n",
        "    text_labels = ['airplane', 'car', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    return [text_labels[int(i)] for i in labels]\n",
        "\n",
        "# defining data_iter, linreg. model, loss, sgd\n",
        "batch_size = 100\n",
        "num_workers = 0\n",
        "train_iter = DataLoader(cifar10_train, batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_iter = DataLoader(cifar10_valid, batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ3lQcSjPG8Q"
      },
      "source": [
        "- cifar10 데이터에 대해 살펴봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVomIvYiJ9OA",
        "outputId": "311ec31d-741d-4aac-af55-e788c7f1a70d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cifar10_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: /content/drive/My Drive/Class/2020-2-bme-dip/data03/\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ColorJitter(brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3])\n",
              "               RandomAffine(degrees=[-10.0, 10.0], scale=(0.99, 1.01), shear=[-10.0, 10.0])\n",
              "               Resize(size=(34, 34), interpolation=PIL.Image.BILINEAR)\n",
              "               CenterCrop(size=(32, 32))\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12YFwLybFdty",
        "outputId": "3732688b-74b0-4674-b68d-503762983934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cifar10_valid"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /content/drive/My Drive/Class/2020-2-bme-dip/data03/\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmGr-Q_h5lOr",
        "outputId": "3a181891-fad6-45d2-b836-f0b743b57fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "dataiter=iter(train_iter)\n",
        "images, labels = dataiter.next()\n",
        "print(labels.shape)\n",
        "print(images.shape)\n",
        "I = images[0].numpy()\n",
        "plt.figure(dpi=100)\n",
        "\n",
        "plt.imshow(np.transpose(I,[1,2,0]),cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(get_cifar10_labels([labels[0]]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100])\n",
            "torch.Size([100, 3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['ship']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNGe9u8uPKE0"
      },
      "source": [
        "- PyTorch에서 제공하는 유명한 CNN architecture를 가져와서 구조를 살펴봅니다.\n",
        "- PyTorch에서는 resnet, densenet 등 유명한 network를 쉽게 불러와서 활용할 수 있습니다. \n",
        "- 여기서는 vgg16 을 가져와 보겠습니다.\n",
        "- https://pytorch.org/hub/research-models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-MVmH1gOCa8",
        "outputId": "6f63b853-8a37-42db-e00e-6ea717bfdca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vgg16 = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=False)\n",
        "vgg16"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufH50VeOPAwL"
      },
      "source": [
        "- 위에서 출력한 network구조를 참고하여 Vgg와 유사한 network를 직접 구현해 봅시다.\n",
        "- 위의 network는 input image가 224 x 224 정도인 경우에 대해 고안된 것이므로, 우리는 32x32의 매우 작은 input image를 다루고 있기 때문에 이를 고려하여야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKS9cociJLcP",
        "outputId": "f43bc166-e4e0-46eb-f179-b41c50025f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    \n",
        "    nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2,stride=2), # 8 x 16 x 16\n",
        "\n",
        "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2,stride=2), # 16 x 8 x 8\n",
        "\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),    \n",
        "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1,padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2,stride=2),  # 32 x 4 x 4\n",
        "\n",
        "    nn.AdaptiveAvgPool2d(output_size = (3, 3)), # 32 x 3 x 3\n",
        "        \n",
        "    Flatten(),\n",
        "\n",
        "    nn.Linear(in_features=32*3*3, out_features=128,bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),    \n",
        "    nn.Linear(in_features=128, out_features=10,bias=True)\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "net = net.to(device)\n",
        "net"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU()\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU()\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU()\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU()\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU()\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU()\n",
              "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (17): AdaptiveAvgPool2d(output_size=(3, 3))\n",
              "  (18): Flatten()\n",
              "  (19): Linear(in_features=2304, out_features=256, bias=True)\n",
              "  (20): ReLU()\n",
              "  (21): Dropout(p=0.5, inplace=False)\n",
              "  (22): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (23): ReLU()\n",
              "  (24): Dropout(p=0.5, inplace=False)\n",
              "  (25): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWBGuPHJPQlm"
      },
      "source": [
        "- 임의로 random tensor를 생성하여, network의 각 layer에 shape을 확인해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9fl7PPRLP5j",
        "outputId": "97d4ff60-2cee-4ce9-d661-8454b7223dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = torch.randn(size=(1,3,32,32), dtype = torch.float32)\n",
        "X = X.to(device)\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv2d output shape: \t torch.Size([1, 64, 32, 32])\n",
            "ReLU output shape: \t torch.Size([1, 64, 32, 32])\n",
            "Conv2d output shape: \t torch.Size([1, 64, 32, 32])\n",
            "ReLU output shape: \t torch.Size([1, 64, 32, 32])\n",
            "MaxPool2d output shape: \t torch.Size([1, 64, 16, 16])\n",
            "Conv2d output shape: \t torch.Size([1, 128, 16, 16])\n",
            "ReLU output shape: \t torch.Size([1, 128, 16, 16])\n",
            "Conv2d output shape: \t torch.Size([1, 128, 16, 16])\n",
            "ReLU output shape: \t torch.Size([1, 128, 16, 16])\n",
            "MaxPool2d output shape: \t torch.Size([1, 128, 8, 8])\n",
            "Conv2d output shape: \t torch.Size([1, 256, 8, 8])\n",
            "ReLU output shape: \t torch.Size([1, 256, 8, 8])\n",
            "Conv2d output shape: \t torch.Size([1, 256, 8, 8])\n",
            "ReLU output shape: \t torch.Size([1, 256, 8, 8])\n",
            "Conv2d output shape: \t torch.Size([1, 256, 8, 8])\n",
            "ReLU output shape: \t torch.Size([1, 256, 8, 8])\n",
            "MaxPool2d output shape: \t torch.Size([1, 256, 4, 4])\n",
            "AdaptiveAvgPool2d output shape: \t torch.Size([1, 256, 3, 3])\n",
            "Flatten output shape: \t torch.Size([1, 2304])\n",
            "Linear output shape: \t torch.Size([1, 256])\n",
            "ReLU output shape: \t torch.Size([1, 256])\n",
            "Dropout output shape: \t torch.Size([1, 256])\n",
            "Linear output shape: \t torch.Size([1, 256])\n",
            "ReLU output shape: \t torch.Size([1, 256])\n",
            "Dropout output shape: \t torch.Size([1, 256])\n",
            "Linear output shape: \t torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5IWjPFXPap3"
      },
      "source": [
        "- Learning rate, num_epochs, loss function, backpropagation algorithm을 정해줍니다.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkDcn5-E-Bf-"
      },
      "source": [
        "# setting hyper-parameters\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "# loss function and algorithm\n",
        "loss = torch.nn.CrossEntropyLoss() # loss\n",
        "alg = torch.optim.SGD(net.parameters(),lr=learning_rate) # sgd"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoaWf0FBPiKb"
      },
      "source": [
        "- 네트워크에 대한 훈련을 진행합니다. 매 10회마다 결과를 출력하게 하였습니다. train, valid accuracy의 변화를 관찰합니다.\n",
        "- 최종결과가 만족스럽지 않은 경우에는 hyper-parameter를 바꾸어가며 다시 학습을 진행합니다.\n",
        "- input data에 대한 random transformation을 추가하는 것은 overfitting에 도움을 줄 수 있습니다.\n",
        "- vggnet이나 resnet과 같이 좀 더 효율적인 네트워크 구조를 적용해 볼 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwFZjo22-Brj"
      },
      "source": [
        "# training the model\n",
        "loss_train = np.array([])\n",
        "accs_train = np.array([])\n",
        "accs_valid = np.array([])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  i=0\n",
        "  l_epoch = 0\n",
        "  correct = 0\n",
        "  for X,y in train_iter:\n",
        "    i=i+1\n",
        "    X,y = X.to(device),y.to(device) # GPU \n",
        "    y_hat=net(X)\n",
        "    correct += (y_hat.argmax(dim=1)==y).sum()\n",
        "    l=loss(y_hat,y)\n",
        "    l_epoch+=l\n",
        "    alg.zero_grad()\n",
        "    l.backward()\n",
        "    alg.step()\n",
        "\n",
        "  loss_train = np.append(loss_train,l_epoch.cpu().detach().numpy()/i)\n",
        "  accs_train = np.append(accs_train,correct.cpu()/50000.)\n",
        "\n",
        "  correct = 0\n",
        "  for X,y in valid_iter:\n",
        "    X,y = X.to(device),y.to(device)\n",
        "    y_hat = net(X)\n",
        "    correct += (y_hat.argmax(dim=1)==y).sum()\n",
        "\n",
        "  accs_valid = np.append(accs_valid,correct.cpu()/10000.)\n",
        "\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    plt.plot(loss_train,label='train loss')\n",
        "    plt.plot(accs_train,label='train accuracy')\n",
        "    plt.plot(accs_valid,label='valid accuracy')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.title('epoch: %d '%(epoch))\n",
        "    plt.pause(.0001)\n",
        "\n",
        "    print('train loss: ',loss_train[-1])\n",
        "    print('train accuracy: ',accs_train[-1])\n",
        "    print('valid accuracy: ',accs_valid[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pvut0zwUsSh"
      },
      "source": [
        "print('valid_accuracy: ',accs_valid[-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65JimMqQQWG"
      },
      "source": [
        "- validation set의 몇몇 example에 대한 결과를 관찰해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YvjZ2MV-BwY"
      },
      "source": [
        "i = 0\n",
        "for X,y in valid_iter:\n",
        "  i=i+1\n",
        "  X=X.to(device)\n",
        "  y_hat = net(X)    \n",
        "  y_hat = y_hat.argmax(dim=1)\n",
        "  for n in range(4):\n",
        "    plt.figure(dpi=100)\n",
        "    plt.imshow(np.transpose(X[n].cpu().squeeze(),[1,2,0]),cmap='gray')\n",
        "    plt.title('label: %s \\n prediction: %s'%(get_cifar10_labels([y[n]]),get_cifar10_labels([y_hat[n]])))\n",
        "    plt.axis('off')\n",
        "    plt.pause(.0001)\n",
        "  if i>2:\n",
        "     break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}